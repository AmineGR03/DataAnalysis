\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{fancyhdr}

% Configuration de la page
\geometry{margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{Formules de Référence}
\fancyfoot[C]{\thepage}

% Configuration des titres
\titleformat{\section}
{\Large\bfseries\color{blue!70!black}}
{}
{0em}
{}[\titlerule]

\titleformat{\subsection}
{\large\bfseries\color{blue!50!black}}
{}
{0em}
{}

\titleformat{\subsubsection}
{\normalsize\bfseries}
{}
{0em}
{}

% Commandes personnalisées
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

% Métadonnées
\title{Formules LaTeX - Référence Rapide\\
\large Analyse de Données et Statistiques}
\author{AmineGR03}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Chapitre 3 : Corrélation et Régression}

\subsection{Coefficient de corrélation de Pearson}

\begin{align}
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
\end{align}

\subsection{Régression linéaire simple}

\subsubsection{Modèle}
\begin{align}
Y = \beta_0 + \beta_1 X + \varepsilon
\end{align}

\subsubsection{Estimateurs}
\begin{align}
\hat{\beta}_1 &= \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1\bar{x}
\end{align}

\subsubsection{Résidus}
\begin{align}
e_i = y_i - \hat{y}_i
\end{align}

\subsubsection{Coefficient de détermination R²}
\begin{align}
R^2 = \frac{SS_{\text{exp}}}{SS_{\text{tot}}} = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
\end{align}

où :
\begin{itemize}
    \item $SS_{\text{tot}} = \sum_{i=1}^{n}(y_i - \bar{y})^2$ : variabilité totale
    \item $SS_{\text{res}} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ : variabilité non expliquée (résiduelle)
    \item $SS_{\text{exp}} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$ : variabilité expliquée
\end{itemize}

\subsection{Régression linéaire multiple}

\subsubsection{Modèle matriciel}
\begin{align}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{align}

où :
\begin{itemize}
    \item $\mathbf{Y}$ : vecteur des observations ($n \times 1$)
    \item $\mathbf{X}$ : matrice des données ($n \times (p+1)$), première colonne = 1 pour l'intercept
    \item $\boldsymbol{\beta}$ : vecteur des coefficients de régression ($(p+1) \times 1$)
    \item $\boldsymbol{\varepsilon}$ : vecteur des erreurs aléatoires ($n \times 1$)
\end{itemize}

\subsubsection{Estimateur MCO (Moindres Carrés Ordinaires)}
\begin{align}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{align}

\subsubsection{R² ajusté}
\begin{align}
R^2_{\text{ajusté}} = 1 - (n-1)\frac{1-R^2}{n-p-1}
\end{align}

où :
\begin{itemize}
    \item $n$ : nombre d'observations
    \item $p$ : nombre de variables explicatives
\end{itemize}

\newpage

\section{Chapitre 5 : Lois de Probabilité}

\subsection{Variables aléatoires discrètes}

\subsubsection{Espérance}
\begin{align}
E(X) = \sum_{x} x \cdot p(x)
\end{align}

\subsubsection{Variance}
\begin{align}
\Var(X) = E[(X - E(X))^2] = E(X^2) - [E(X)]^2
\end{align}

où :
\begin{align}
E(X^2) = \sum_{x} x^2 \cdot p(x)
\end{align}

\subsubsection{Écart-type}
\begin{align}
\sigma(X) = \sqrt{\Var(X)}
\end{align}

\subsection{Variables aléatoires continues}

\subsubsection{Probabilité sur un intervalle}
\begin{align}
P(a \leq X \leq b) = \int_{a}^{b} f(x)\,dx
\end{align}

\subsubsection{Espérance}
\begin{align}
E(X) = \int_{-\infty}^{+\infty} x \cdot f(x)\,dx
\end{align}

\subsubsection{Variance}
\begin{align}
\Var(X) = \int_{-\infty}^{+\infty} (x - E(X))^2 f(x)\,dx = E(X^2) - [E(X)]^2
\end{align}

où :
\begin{align}
E(X^2) = \int_{-\infty}^{+\infty} x^2 \cdot f(x)\,dx
\end{align}

\subsubsection{Écart-type}
\begin{align}
\sigma(X) = \sqrt{\Var(X)}
\end{align}

\subsection{Loi de Bernoulli}

\subsubsection{Fonction de masse de probabilité}
\begin{align}
P(X = x) = p^x(1-p)^{1-x}, \quad x \in \{0, 1\}
\end{align}

ou de manière équivalente :
\begin{itemize}
    \item $P(X = 1) = p$
    \item $P(X = 0) = 1-p$
\end{itemize}

\subsubsection{Espérance et variance}
\begin{align}
E(X) = p, \quad \Var(X) = p(1-p)
\end{align}

\subsubsection{Notation}
\[
X \sim \text{Ber}(p)
\]

\subsection{Loi Binomiale}

\subsubsection{Fonction de masse de probabilité}
\begin{align}
P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}, \quad k = 0, 1, 2, \ldots, n
\end{align}

où le coefficient binomial est :
\begin{align}
\binom{n}{k} = \frac{n!}{k!(n-k)!}
\end{align}

\subsubsection{Espérance et variance}
\begin{align}
E(X) = np, \quad \Var(X) = np(1-p)
\end{align}

\subsubsection{Notation}
\[
X \sim B(n, p)
\]

\subsection{Loi Exponentielle}

\subsubsection{Fonction de densité de probabilité}
\begin{align}
f_X(x) = \lambda e^{-\lambda x}, \quad x \geq 0
\end{align}

\subsubsection{Fonction de répartition}
\begin{align}
F_X(x) = 1 - e^{-\lambda x}, \quad x \geq 0
\end{align}

\subsubsection{Espérance et variance}
\begin{align}
E(X) = \frac{1}{\lambda}, \quad \Var(X) = \frac{1}{\lambda^2}
\end{align}

\subsubsection{Propriété importante : Absence de mémoire}
\begin{align}
P(X > s + t | X > s) = P(X > t)
\end{align}

\subsubsection{Notation}
\[
X \sim \text{Exp}(\lambda)
\]

\subsection{Loi Normale (Gaussienne)}

\subsubsection{Fonction de densité de probabilité}
\begin{align}
f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right), \quad x \in \R
\end{align}

\subsubsection{Propriétés}
\begin{itemize}
    \item \textbf{Symétrie} : La courbe est symétrique par rapport à la moyenne $\mu$
    \item \textbf{Espérance et variance} :
    \begin{align}
    E(X) = \mu, \quad \Var(X) = \sigma^2
    \end{align}
\end{itemize}

\subsubsection{Fonction de répartition}
\begin{align}
F_X(x) = P(X \leq x) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{x-\mu}{\sigma\sqrt{2}}\right)\right]
\end{align}

où la fonction d'erreur est :
\begin{align}
\text{erf}(z) = \frac{2}{\sqrt{\pi}} \int_0^z e^{-t^2} \, dt
\end{align}

\subsubsection{Notation}
\[
X \sim N(\mu, \sigma^2)
\]

\subsection{Loi Normale Centrée Réduite}

\subsubsection{Définition}
Si $X \sim N(\mu, \sigma^2)$, alors la variable centrée réduite :
\begin{align}
Z = \frac{X - \mu}{\sigma} \sim N(0, 1)
\end{align}

\subsubsection{Fonction de densité}
\begin{align}
\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}, \quad z \in \R
\end{align}

\subsubsection{Fonction de répartition}
\begin{align}
\Phi(z) = P(Z \leq z) = \int_{-\infty}^{z} \phi(t) \, dt
\end{align}

\subsubsection{Propriétés importantes}
\begin{itemize}
    \item $\Phi(-z) = 1 - \Phi(z)$
    \item $P(a \leq Z \leq b) = \Phi(b) - \Phi(a)$
    \item Pour $X \sim N(\mu, \sigma^2)$ : $P(a \leq X \leq b) = \Phi\left(\frac{b-\mu}{\sigma}\right) - \Phi\left(\frac{a-\mu}{\sigma}\right)$
\end{itemize}

\subsubsection{Règle des 3 sigmas}
Pour $X \sim N(\mu, \sigma^2)$ :
\begin{itemize}
    \item $P(\mu - \sigma \leq X \leq \mu + \sigma) \approx 0.68$ (68\%)
    \item $P(\mu - 2\sigma \leq X \leq \mu + 2\sigma) \approx 0.95$ (95\%)
    \item $P(\mu - 3\sigma \leq X \leq \mu + 3\sigma) \approx 0.997$ (99.7\%)
\end{itemize}

\newpage

\section{Chapitre 5 (Suite) : Tests d'Hypothèses}

\subsection{Test sur la moyenne (variance connue)}

\subsubsection{Statistique de test}
\begin{align}
Z_0 = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0,1)
\end{align}

Si $H_0$ est vraie, alors $Z_0 \sim N(0,1)$ (pour $n$ grand ou si $X \sim N(\mu, \sigma^2)$).

\subsubsection{Règle de décision bilatérale}
\begin{itemize}
    \item \textbf{Rejeter $H_0$} si $|Z_0| > z_{\alpha/2}$
    \item \textbf{Accepter $H_0$} si $|Z_0| \leq z_{\alpha/2}$
\end{itemize}

où $z_{\alpha/2}$ est le quantile d'ordre $1-\alpha/2$ de la loi normale standard.

\textbf{Valeurs critiques usuelles} :
\begin{itemize}
    \item $\alpha = 0.05$ : $z_{0.025} = 1.96$
    \item $\alpha = 0.01$ : $z_{0.005} = 2.576$
\end{itemize}

\subsection{Test sur la moyenne (variance inconnue)}

\subsubsection{Statistique de test}
\begin{align}
T_0 = \frac{\bar{X} - \mu_0}{S/\sqrt{n}} \sim t_{n-1}
\end{align}

où $S$ est l'écart-type empirique :
\begin{align}
S = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}
\end{align}

Si $H_0$ est vraie et $X \sim N(\mu, \sigma^2)$, alors $T_0 \sim t_{n-1}$ (loi de Student à $n-1$ degrés de liberté).

\subsubsection{Règle de décision bilatérale}
\begin{itemize}
    \item \textbf{Rejeter $H_0$} si $|T_0| > t_{\alpha/2; n-1}$
    \item \textbf{Accepter $H_0$} si $|T_0| \leq t_{\alpha/2; n-1}$
\end{itemize}

où $t_{\alpha/2; n-1}$ est le quantile d'ordre $1-\alpha/2$ de la loi de Student à $n-1$ degrés de liberté.

\subsection{Tests unilatéraux}

\subsubsection{Test unilatéral à gauche}
\textbf{Hypothèses} :
\[
H_0 : \mu = \mu_0 \text{ (ou } \mu \geq \mu_0\text{)}, \quad H_1 : \mu < \mu_0
\]

\textbf{Règle de rejet} :
\begin{itemize}
    \item Si $\sigma$ connu : rejeter $H_0$ si $Z_0 < -z_{\alpha}$
    \item Si $\sigma$ inconnu : rejeter $H_0$ si $T_0 < -t_{\alpha; n-1}$
\end{itemize}

\subsubsection{Test unilatéral à droite}
\textbf{Hypothèses} :
\[
H_0 : \mu = \mu_0 \text{ (ou } \mu \leq \mu_0\text{)}, \quad H_1 : \mu > \mu_0
\]

\textbf{Règle de rejet} :
\begin{itemize}
    \item Si $\sigma$ connu : rejeter $H_0$ si $Z_0 > z_{\alpha}$
    \item Si $\sigma$ inconnu : rejeter $H_0$ si $T_0 > t_{\alpha; n-1}$
\end{itemize}

\subsection{Niveau critique observé (P-value)}

\subsubsection{Définition}
Le \textbf{niveau critique observé} (ou \textbf{P-value}, notée $PV$) est la valeur minimale de $\alpha$ telle que $H_0$ est toujours rejetée.

\subsubsection{Calcul de la P-value}

\textbf{Variance connue (bilatéral)} :
\begin{align}
PV = 2(1 - \Phi(|z_0|))
\end{align}

\textbf{Variance inconnue (bilatéral)} :
\begin{align}
PV = 2P(T > |t_0|) \quad \text{avec } T \sim t_{n-1}
\end{align}

\subsubsection{Interprétation}
\begin{itemize}
    \item Si $PV < \alpha$ : rejeter $H_0$
    \item Si $PV \geq \alpha$ : ne pas rejeter $H_0$
\end{itemize}

\subsection{Test d'ajustement du khi-deux ($\chi^2$)}

\subsubsection{Statistique de test}
\begin{align}
\chi_0^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i} \sim \chi_{k-p-1}^2
\end{align}

où :
\begin{itemize}
    \item $k$ : nombre de classes
    \item $p$ : nombre de paramètres estimés
    \item $O_i$ : effectifs observés
    \item $E_i$ : effectifs attendus
\end{itemize}

\subsubsection{Effectifs attendus}
\begin{align}
E_i = n \times p_i^{(0)}
\end{align}

où :
\begin{align}
p_i^{(0)} = P(X \in V_i | H_0 \text{ est vraie})
\end{align}

\subsubsection{Règle de décision}
Pour un niveau critique $\alpha$ donné :
\begin{itemize}
    \item \textbf{Rejeter $H_0$} si $\chi_0^2 > \chi_{\alpha; \nu}^2$
    \item \textbf{Accepter $H_0$} si $\chi_0^2 \leq \chi_{\alpha; \nu}^2$
\end{itemize}

où $\chi_{\alpha; \nu}^2$ est le quantile d'ordre $1-\alpha$ de la loi $\chi_{\nu}^2$ avec $\nu = k-p-1$.

\newpage

\section{Chapitre 6 : Analyse en Composantes Principales (ACP)}

\subsection{Matrices}

\subsubsection{Matrice centrée}
\begin{align}
a_{ij} = x_{ij} - \bar{x}_j
\end{align}

\subsubsection{Matrice centrée réduite}
\begin{align}
m_{ij} &= \frac{x_{ij} - \bar{x}_j}{\sigma_j} \\
\sigma_j &= \sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij} - \bar{x}_j)^2}
\end{align}

où :
\begin{itemize}
    \item $\bar{x}_j$ : moyenne de la variable $X_j$
    \item $\sigma_j$ : écart-type de la variable $X_j$
\end{itemize}

\subsubsection{Matrice des variances-covariances}
\begin{align}
\mathbf{V} = \frac{1}{n} \mathbf{M}_c^T \mathbf{M}_c
\end{align}

où $\mathbf{M}_c^T$ est la transposée de la matrice centrée.

\subsubsection{Matrice des corrélations}
\begin{align}
\mathbf{U} = \frac{1}{n} \mathbf{M}_{cr}^T \mathbf{M}_{cr}
\end{align}

où $\mathbf{M}_{cr}^T$ est la transposée de la matrice centrée réduite.

\subsection{Inertie}

\subsubsection{Calcul}
\begin{align}
I_g = \frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{p}(x_{ik} - \bar{x}_k)^2 = \sum_{k=1}^{p} \sigma_k^2
\end{align}

\subsubsection{Cas des données centrées-réduites}
Si les données sont centrées et réduites :
\begin{itemize}
    \item $I_g = p$
    \item $\mathbf{g}^\top = (0, 0, \ldots, 0)$
\end{itemize}

\subsection{Valeurs propres et vecteurs propres}

\subsubsection{Équation caractéristique}
Pour chaque valeur propre $\lambda_k$, on trouve le vecteur propre associé $\mathbf{u}_k$ tel que :
\begin{align}
\mathbf{U}\mathbf{u}_k = \lambda_k \mathbf{u}_k
\end{align}

\subsubsection{Polynôme caractéristique}
\begin{align}
P(\lambda) = \det(\mathbf{U} - \lambda \mathbf{I}_p) = 0
\end{align}

\subsubsection{Ordre des valeurs propres}
Les valeurs propres $\lambda_1, \lambda_2, \ldots, \lambda_p$ sont ordonnées par ordre décroissant :
\begin{align}
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0
\end{align}

\subsection{Composantes principales}

\subsubsection{Calcul}
\begin{align}
\mathbf{C}_1 &= \mathbf{M}_{cr} \mathbf{u}_1 \\
\mathbf{C}_2 &= \mathbf{M}_{cr} \mathbf{u}_2 \\
&\vdots \\
\mathbf{C}_k &= \mathbf{M}_{cr} \mathbf{u}_k
\end{align}

\subsubsection{Propriétés}
\begin{itemize}
    \item \textbf{Centrées} : la moyenne de chaque composante est nulle
    \item \textbf{Variance} : la variance de la composante $C_k$ est égale à la valeur propre $\lambda_k$
    \item \textbf{Non corrélées} : les composantes principales sont deux à deux non corrélées
\end{itemize}

\subsection{Inertie expliquée}

\subsubsection{Inertie d'un axe}
\begin{align}
\text{Inertie axe } k = \frac{\lambda_k}{\sum_{i=1}^{p} \lambda_i} = \frac{\lambda_k}{I_g}
\end{align}

Pour des données centrées-réduites ($I_g = p$) :
\begin{align}
\text{Inertie axe } k = \frac{\lambda_k}{p}
\end{align}

\subsubsection{Inertie cumulée}
L'inertie cumulée des $k$ premiers axes est :
\begin{align}
\text{Inertie cumulée} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{p} \lambda_i}
\end{align}

\subsection{Contributions}

\subsubsection{Contribution d'un individu à l'inertie totale}
\begin{align}
\text{CONTR}(i) = \frac{1}{n} \sum_{j=1}^{p} \frac{m_{ij}^2}{I_g}
\end{align}

Pour des données centrées-réduites ($I_g = p$) :
\begin{align}
\text{CONTR}(i) = \frac{1}{np} \sum_{j=1}^{p} m_{ij}^2
\end{align}

\subsubsection{Contribution d'une variable}
\begin{align}
\text{CONTR}(X_j) = \frac{1}{p} \sum_{i=1}^{n} \frac{m_{ij}^2}{I_g}
\end{align}

\subsubsection{Contribution d'un individu à la construction d'un axe}
\begin{align}
\text{CONTR}(i, \text{axe } k) = \frac{1}{n} \frac{C_{ik}^2}{\lambda_k}
\end{align}

où $C_{ik}$ est la coordonnée de l'individu $i$ sur la composante principale $C_k$.

\newpage

\section{Opérations sur matrices}

\subsection{Multiplication matricielle}

Pour $\mathbf{A}$ ($m \times n$) et $\mathbf{B}$ ($n \times p$) :
\begin{align}
(\mathbf{AB})_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
\end{align}

\subsection{Transposée}

\begin{align}
(\mathbf{A}^T)_{ij} = a_{ji}
\end{align}

\subsection{Propriétés importantes}

\begin{itemize}
    \item $(\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T$
    \item $(\mathbf{A}^T)^T = \mathbf{A}$
    \item $(\mathbf{A}^{-1})^T = (\mathbf{A}^T)^{-1}$
    \item $(\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}$ (si les inverses existent)
    \item Si $\mathbf{A}$ est symétrique : $\mathbf{A}^T = \mathbf{A}$
\end{itemize}

\subsection{Déterminant}

\subsubsection{Matrice 2×2}
\begin{align}
\det(\mathbf{A}) = \begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc
\end{align}

\subsubsection{Matrice 3×3}
\begin{align}
\det(\mathbf{A}) = a(ei - fh) - b(di - fg) + c(dh - eg)
\end{align}

où $\mathbf{A} = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}$.

\newpage

\section*{Notes d'utilisation}

\addcontentsline{toc}{section}{Notes d'utilisation}

\begin{itemize}
    \item Utilisez \texttt{\textbackslash begin\{align\}...\textbackslash end\{align\}} pour les équations multi-lignes
    \item Utilisez \texttt{\textbackslash text\{\}} pour le texte dans les formules mathématiques
    \item Utilisez \texttt{\textbackslash ,} pour un espacement fin dans les intégrales
    \item Les indices utilisent \texttt{\_} et les exposants \texttt{\^{}}
    \item Les vecteurs en gras utilisent \texttt{\textbackslash mathbf\{\}} ou \texttt{\textbackslash boldsymbol\{\}}
    \item Les matrices utilisent \texttt{\textbackslash mathbf\{\}} ou \texttt{\textbackslash boldsymbol\{\}}
    \item Les ensembles de nombres utilisent \texttt{\textbackslash mathbb\{\}} (ex: $\R$, $\N$)
\end{itemize}

\end{document}

